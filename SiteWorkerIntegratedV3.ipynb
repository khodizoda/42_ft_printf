{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SiteWorkerIntegratedV3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khodizoda/42_ft_printf/blob/master/SiteWorkerIntegratedV3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIuKcSVRxOAr"
      },
      "source": [
        " ## **Set up and update**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQdKEHfAmSOv"
      },
      "source": [
        "# from google.colab import auth\n",
        "# credentials = auth.authenticate_user()\n",
        "\n",
        "!apt-get update && apt-get upgrade\n",
        "!apt install chromium-chromedriver\n",
        "!pip install centaurMiner==0.0.8\n",
        "!pip install import_ipynb\n",
        "!pip install tld\n",
        "\n",
        "!apt autoremove\n",
        "\n",
        "!curl 'https://raw.githubusercontent.com/aivscovid19/data_pipeline/gulnoza/JobDispatcher.ipynb' > JobDispatcher.ipynb\n",
        "!curl 'https://raw.githubusercontent.com/khodizoda/ai_vs_covid19/master/IbmcRuMiner.ipynb' > IbmcRuMiner.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP-sTZTfKsR8"
      },
      "source": [
        "import import_ipynb\n",
        "import random, time\n",
        "import pandas as pd\n",
        "from pandas.io import gbq\n",
        "from tld import get_fld\n",
        "from JobDispatcher import JobDispatcher\n",
        "from IbmcRuMiner import IbmcRuMiner\n",
        "\n",
        "class SiteWorkerIntegrated():\n",
        "  '''\n",
        "  SiteWorkerIntegrated class uses `site_worker_factory` method\n",
        "  to send data mining request to a domain-specific SiteWorker\n",
        "  given urls dataframe.\n",
        "\n",
        "  Attributes:\n",
        "    max_threshold (int): A default value of how many articles to upload\n",
        "                          to a BigQuery table at a time.\n",
        "    min_delay (int): A default value of min seconds to wait before the\n",
        "                      next request to a website is sent.\n",
        "    max_delay (int): A default value of max seconds to wait before the\n",
        "                      next request to a website is sent.\n",
        "  '''\n",
        "\n",
        "  def __init__(self):\n",
        "    self.max_threshold = 50\n",
        "    self.min_delay = 0.1\n",
        "    self.max_delay = 2\n",
        "\n",
        "  def send_request(self, urls_df, limit):\n",
        "    ''' Finds domain from urls dataframe and sends request\n",
        "    to a domain-specific SiteWorker using site_worker_factory class method.\n",
        "    '''\n",
        "    try:\n",
        "      url = urls_df.at[0, 'article_url'] \n",
        "      domain = get_fld(url)\n",
        "      print(domain)\n",
        "\n",
        "      self.site_worker_factory(domain, urls_df, limit, self.driver_path)\n",
        "  \n",
        "    except:\n",
        "      print('The urls dataframe is empty')\n",
        "\n",
        "  def scrape_data(self, miner, urls_df, article_schema, limit=100):\n",
        "    ''' Scrapes data given urls' dataframe, and limit of articles to scrape.\n",
        "    Uploads data to a given BigQuery table and calls `update_job_status` method\n",
        "    from `JobDispatcher` class from `JobDispatcher` module\n",
        "    to update `status` of the job to `done`.\n",
        "\n",
        "    Attributes:\n",
        "      urls_df (pandas dataframe): A urls dataframe, to get a list of\n",
        "                                    article urls to scrape.\n",
        "      limit (int): A limit of articles to scrape. Default is 100.\n",
        "    '''\n",
        "\n",
        "    urls = [url for url in list(urls_df['article_url'])]\n",
        "    data = []\n",
        "    prev_count = 0\n",
        "    for count, url in enumerate(urls, 1):\n",
        "      miner.gather(url)\n",
        "      data.append(miner.results)\n",
        "      if (count == self.max_threshold or count == limit or count == len(urls)):\n",
        "        articles_list = list(filter(lambda i:\n",
        "                            i['title'] != None and len(i['title']) != 0 and\n",
        "                            i['abstract'] != None and len(i['abstract']) != 0,\n",
        "                            data[prev_count : count])) \n",
        "        articles_df = pd.DataFrame(articles_list)\n",
        "        articles_df.to_gbq(destination_table=f'{self.article_table}',\n",
        "                  project_id=self.project_id,\n",
        "                  if_exists='append',\n",
        "                  table_schema=article_schema,\n",
        "                  credentials=self.credentials)\n",
        "        JobDispatcher(self.credentials,\n",
        "                      self.project_id,\n",
        "                      self.url_table).update_job_status(urls_df.iloc[prev_count : count].copy())\n",
        "        prev_count = count\n",
        "      time.sleep(self.min_delay + self.max_delay * random.random())\n",
        "    return articles_df\n",
        "  \n",
        "  @classmethod\n",
        "  def init(cls, credentials, project_id, url_table, article_table, driver_path=None):\n",
        "    ''' Initializes `SiteWorkerIngrated` class\n",
        "\n",
        "    Attributes:\n",
        "      credentials (str): Credentials, either from user_account or service_account,\n",
        "                          to authenticate to Google Cloud APIs.\n",
        "      project_id (str): A project_id on Google Cloud Platform.\n",
        "      url_table (str): A url_table to use to retrieve urls_dataframe from,\n",
        "                        in form of `dataset_id.table_id`.\n",
        "      article_table (str): An article_table to use to upload scraped data to,\n",
        "                        in form of `dataset_id.table_id`.\n",
        "      driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    '''\n",
        "    cls.credentials = credentials\n",
        "    cls.project_id = project_id\n",
        "    cls.url_table = url_table\n",
        "    cls.article_table = article_table\n",
        "    cls.driver_path = driver_path\n",
        "\n",
        "  @classmethod\n",
        "  def site_worker_factory(cls, domain_name, urls_df, limit=100, driver_path=None):\n",
        "    ''' Sends a scraping request to a domain-specific SiteWorker\n",
        "\n",
        "    Attributes:\n",
        "      domain_name (str): A domain.\n",
        "      urls_df (pandas dataframe): A urls dataframe.\n",
        "      limit (int): A limit of articles to scrape. Default is 100.\n",
        "      driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    '''\n",
        "    site_worker = {'ibmc.msk.ru': IbmcRuSiteWorker(urls_df, limit, driver_path).scrape_articles()\n",
        "    \n",
        "    }"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEN1V06rBaLy"
      },
      "source": [
        "class IbmcRuSiteWorker(SiteWorkerIntegrated):\n",
        "  '''\n",
        "  SiteWorker for http://pbmc.ibmc.msk.ru/\n",
        "\n",
        "  Attributes:\n",
        "    urls_df (pandas dataframe): A urls dataframe.\n",
        "    limit (int): A limit of articles to scrape. Default is 100.\n",
        "    driver_path (str): A driver path to a chromium-chromedriver.\n",
        "    article_schema (list of dicts): An default article_schema for\n",
        "                                      a given SiteWorker.\n",
        "  '''\n",
        "  def __init__(self, urls_df, limit=100, driver_path=None):\n",
        "    super().__init__()\n",
        "    self.urls_df = urls_df\n",
        "    self.limit = limit\n",
        "    self.driver_path = driver_path\n",
        "    self.article_schema = [\n",
        "      {'name': 'abstract',                'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "      {'name': 'authors',                 'type': 'STRING'                    },\n",
        "      {'name': 'date_publication',        'type': 'DATE'                      },\n",
        "      {'name': 'doi',                     'type': 'STRING'                    },\n",
        "      {'name': 'extra_link',              'type': 'STRING'                    },\n",
        "      {'name': 'keywords',                'type': 'STRING'                    },\n",
        "      {'name': 'license',                 'type': 'STRING'                    },\n",
        "      {'name': 'organization_affiliated', 'type': 'STRING'                    },\n",
        "      {'name': 'pubmed_link',             'type': 'STRING'                    },\n",
        "      {'name': 'source',                  'type': 'STRING'                    },\n",
        "      {'name': 'title',                   'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "      {'name': 'translated_link',         'type': 'STRING',                   },\n",
        "      {'name': 'url',                     'type': 'STRING', 'mode': 'REQUIRED'},\n",
        "      {'name': 'date_aquisition',         'type': 'DATE'                      },           \n",
        "    ]\n",
        "\n",
        "  def scrape_articles(self):\n",
        "    miner = IbmcRuMiner.IbmcEngine(IbmcRuMiner.IbmcLocations, driver_path=self.driver_path)\n",
        "    self.scrape_data(miner, self.urls_df, self.article_schema, self.limit)"
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}